{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD Object Detection –¥–ª—è BCCD Dataset\n",
    "\n",
    "–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Single Shot MultiBox Detector (SSD300) –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∫–ª–µ—Ç–æ–∫ –∫—Ä–æ–≤–∏.\n",
    "\n",
    "**–î–∞—Ç–∞—Å–µ—Ç:** [BCCD Dataset](https://github.com/Shenggan/BCCD_Dataset)\n",
    "\n",
    "**–†–µ—Ñ–µ—Ä–µ–Ω—Å—ã:**\n",
    "- [PyTorch Tutorial to Object Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)\n",
    "- [D2L.ai SSD Chapter](https://d2l.ai/chapter_computer-vision/ssd.html)\n",
    "\n",
    "**–ö–ª–∞—Å—Å—ã –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏:**\n",
    "- WBC (White Blood Cells) - –õ–µ–π–∫–æ—Ü–∏—Ç—ã\n",
    "- RBC (Red Blood Cells) - –≠—Ä–∏—Ç—Ä–æ—Ü–∏—Ç—ã\n",
    "- Platelets - –¢—Ä–æ–º–±–æ—Ü–∏—Ç—ã\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è –í–∞–∂–Ω–æ: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "\n",
    "**–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º notebook —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:**\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision pillow matplotlib opencv-python tqdm lxml numpy\n",
    "```\n",
    "\n",
    "**–ò–ª–∏ –¥–ª—è Google Colab –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —è—á–µ–π–∫—É –Ω–∏–∂–µ.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –¢–û–õ–¨–ö–û –î–õ–Ø GOOGLE COLAB\n",
    "# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ Colab\n",
    "# ============================================\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -q torch torchvision pillow matplotlib opencv-python tqdm lxml numpy\n",
    "# print(\"‚úÖ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –¥–ª—è Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ò–º–ø–æ—Ä—Ç—ã –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –û–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –Ω–∞ CPU (–º–µ–¥–ª–µ–Ω–Ω–æ).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º\n",
    "DATA_ROOT = './BCCD_Dataset/BCCD'\n",
    "ANNOTATIONS_DIR = os.path.join(DATA_ROOT, 'Annotations')\n",
    "IMAGES_DIR = os.path.join(DATA_ROOT, 'JPEGImages')\n",
    "IMAGESETS_DIR = os.path.join(DATA_ROOT, 'ImageSets/Main')\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "IMAGE_SIZE = 300\n",
    "NUM_CLASSES = 4  # background + 3 –∫–ª–∞—Å—Å–∞ (WBC, RBC, Platelets)\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "GRAD_CLIP = 10.0\n",
    "\n",
    "# Prior boxes –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "FEATURE_MAPS = [38, 19, 10, 5, 3, 1]\n",
    "OBJ_SCALES = [0.1, 0.2, 0.375, 0.55, 0.725, 0.9]\n",
    "ASPECT_RATIOS = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ç–µ–∫—Ü–∏–∏\n",
    "IOU_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.45\n",
    "CONFIDENCE_THRESHOLD = 0.01\n",
    "TOP_K = 200\n",
    "\n",
    "# –ö–ª–∞—Å—Å—ã\n",
    "LABEL_MAP = {'background': 0, 'WBC': 1, 'RBC': 2, 'Platelets': 3}\n",
    "REV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "COLORS = {\n",
    "    'WBC': (255, 0, 0),\n",
    "    'RBC': (0, 255, 0),\n",
    "    'Platelets': (0, 0, 255)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "\n",
    "**–ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:**\n",
    "```bash\n",
    "git clone https://github.com/Shenggan/BCCD_Dataset.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    print(\"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "    print(\"–í—ã–ø–æ–ª–Ω–∏—Ç–µ: git clone https://github.com/Shenggan/BCCD_Dataset.git\")\n",
    "    raise FileNotFoundError(f\"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {DATA_ROOT}\")\n",
    "else:\n",
    "    print(\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –Ω–∞–π–¥–µ–Ω\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    num_images = len(list(Path(IMAGES_DIR).glob('*.jpg')))\n",
    "    num_annotations = len(list(Path(ANNOTATIONS_DIR).glob('*.xml')))\n",
    "    \n",
    "    print(f\"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {num_images}\")\n",
    "    print(f\"  –ê–Ω–Ω–æ—Ç–∞—Ü–∏–π: {num_annotations}\")\n",
    "    \n",
    "    # –ü–æ–¥—Å—á–µ—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º\n",
    "    class_counts = {'WBC': 0, 'RBC': 0, 'Platelets': 0}\n",
    "    \n",
    "    for xml_file in Path(ANNOTATIONS_DIR).glob('*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.findall('object'):\n",
    "            class_name = obj.find('name').text\n",
    "            if class_name in class_counts:\n",
    "                class_counts[class_name] += 1\n",
    "    \n",
    "    print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"  {cls}: {count}\")\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(class_counts.keys(), class_counts.values(), color=['red', 'green', 'blue'])\n",
    "    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ BCCD', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('–ö–ª–∞—Å—Å')\n",
    "    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    for i, (cls, count) in enumerate(class_counts.items()):\n",
    "        plt.text(i, count + 20, str(count), ha='center', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è Prior Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prior_boxes():\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–Ω–∏–µ prior (anchor) boxes –¥–ª—è SSD300.\n",
    "    \n",
    "    Returns:\n",
    "        prior_boxes: Tensor —Ä–∞–∑–º–µ—Ä–∞ (8732, 4) —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ [cx, cy, w, h]\n",
    "    \"\"\"\n",
    "    fmap_dims = FEATURE_MAPS\n",
    "    obj_scales = OBJ_SCALES\n",
    "    aspect_ratios = ASPECT_RATIOS\n",
    "    \n",
    "    prior_boxes = []\n",
    "    \n",
    "    for k, fmap_dim in enumerate(fmap_dims):\n",
    "        for i in range(fmap_dim):\n",
    "            for j in range(fmap_dim):\n",
    "                cx = (j + 0.5) / fmap_dim\n",
    "                cy = (i + 0.5) / fmap_dim\n",
    "                \n",
    "                # Aspect ratio 1:1\n",
    "                scale = obj_scales[k]\n",
    "                prior_boxes.append([cx, cy, scale, scale])\n",
    "                \n",
    "                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π scale –¥–ª—è aspect ratio 1:1\n",
    "                if k < len(fmap_dims) - 1:\n",
    "                    scale_next = math.sqrt(scale * obj_scales[k + 1])\n",
    "                else:\n",
    "                    scale_next = 1.0\n",
    "                prior_boxes.append([cx, cy, scale_next, scale_next])\n",
    "                \n",
    "                # –î—Ä—É–≥–∏–µ aspect ratios\n",
    "                for ar in aspect_ratios[k]:\n",
    "                    prior_boxes.append([cx, cy, scale * math.sqrt(ar), scale / math.sqrt(ar)])\n",
    "                    prior_boxes.append([cx, cy, scale / math.sqrt(ar), scale * math.sqrt(ar)])\n",
    "    \n",
    "    prior_boxes = torch.FloatTensor(prior_boxes).to(device)\n",
    "    prior_boxes.clamp_(0, 1)\n",
    "    \n",
    "    return prior_boxes\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ prior boxes\n",
    "prior_boxes = create_prior_boxes()\n",
    "print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–æ {prior_boxes.size(0)} prior boxes\")\n",
    "print(f\"   Shape: {prior_boxes.shape}\")\n",
    "print(f\"   Device: {prior_boxes.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_to_cxcy(xy):\n",
    "    \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ (xmin, ymin, xmax, ymax) –≤ (cx, cy, w, h)\"\"\"\n",
    "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2, xy[:, 2:] - xy[:, :2]], 1)\n",
    "\n",
    "def cxcy_to_xy(cxcy):\n",
    "    \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ (cx, cy, w, h) –≤ (xmin, ymin, xmax, ymax)\"\"\"\n",
    "    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2), cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)\n",
    "\n",
    "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
    "    \"\"\"–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ bounding boxes –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ prior boxes\"\"\"\n",
    "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),\n",
    "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)\n",
    "\n",
    "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
    "    \"\"\"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ bounding boxes –∏–∑ offsets\"\"\"\n",
    "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],\n",
    "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)\n",
    "\n",
    "def find_intersection(set_1, set_2):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–±–æ—Ä–∞–º–∏ boxes\"\"\"\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]\n",
    "\n",
    "def find_jaccard_overlap(set_1, set_2):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ IoU –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–±–æ—Ä–∞–º–∏ boxes\"\"\"\n",
    "    intersection = find_intersection(set_1, set_2)\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection\n",
    "    return intersection / union\n",
    "\n",
    "print(\"‚úÖ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è bbox –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset –∫–ª–∞—Å—Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCCDDataset(Dataset):\n",
    "    def __init__(self, data_folder, split='train', transform=None):\n",
    "        self.split = split.upper()\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # –ß—Ç–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤\n",
    "        split_file = os.path.join(IMAGESETS_DIR, f'{split}.txt')\n",
    "        \n",
    "        if not os.path.exists(split_file):\n",
    "            raise FileNotFoundError(f\"Split file not found: {split_file}\")\n",
    "        \n",
    "        with open(split_file, 'r') as f:\n",
    "            self.ids = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        print(f\"Loaded {len(self.ids)} images for {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.ids[idx]\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        image_path = os.path.join(IMAGES_DIR, f'{image_id}.jpg')\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # –ü–∞—Ä—Å–∏–Ω–≥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "        annotation_path = os.path.join(ANNOTATIONS_DIR, f'{image_id}.xml')\n",
    "        boxes, labels = self.parse_annotation(annotation_path)\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π\n",
    "        if self.transform:\n",
    "            image, boxes, labels = self.transform(image, boxes, labels)\n",
    "        \n",
    "        return image, boxes, labels\n",
    "    \n",
    "    def parse_annotation(self, xml_file):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        size = root.find('size')\n",
    "        width = float(size.find('width').text)\n",
    "        height = float(size.find('height').text)\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            class_name = obj.find('name').text\n",
    "            if class_name not in LABEL_MAP:\n",
    "                continue\n",
    "            \n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = float(bbox.find('xmin').text) / width\n",
    "            ymin = float(bbox.find('ymin').text) / height\n",
    "            xmax = float(bbox.find('xmax').text) / width\n",
    "            ymax = float(bbox.find('ymax').text) / height\n",
    "            \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(LABEL_MAP[class_name])\n",
    "        \n",
    "        return torch.FloatTensor(boxes), torch.LongTensor(labels)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images = []\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "        \n",
    "        images = torch.stack(images, dim=0)\n",
    "        return images, boxes, labels\n",
    "\n",
    "print(\"‚úÖ BCCDDataset –∫–ª–∞—Å—Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform:\n",
    "    def __init__(self, size=300, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, image, boxes, labels):\n",
    "        # Resize –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        image = transforms.functional.resize(image, (self.size, self.size))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = transforms.functional.normalize(image, mean=self.mean, std=self.std)\n",
    "        \n",
    "        return image, boxes, labels\n",
    "\n",
    "print(\"‚úÖ Transform –∫–ª–∞—Å—Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ SSD300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    \"\"\"VGG-16 base network –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\"\"\"\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π VGG16\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        \n",
    "        # –°–ª–æ–∏ VGG16 –¥–æ pool5\n",
    "        self.conv1_1 = vgg16.features[0]\n",
    "        self.conv1_2 = vgg16.features[2]\n",
    "        self.pool1 = vgg16.features[4]\n",
    "        \n",
    "        self.conv2_1 = vgg16.features[5]\n",
    "        self.conv2_2 = vgg16.features[7]\n",
    "        self.pool2 = vgg16.features[9]\n",
    "        \n",
    "        self.conv3_1 = vgg16.features[10]\n",
    "        self.conv3_2 = vgg16.features[12]\n",
    "        self.conv3_3 = vgg16.features[14]\n",
    "        self.pool3 = vgg16.features[16]\n",
    "        \n",
    "        self.conv4_1 = vgg16.features[17]\n",
    "        self.conv4_2 = vgg16.features[19]\n",
    "        self.conv4_3 = vgg16.features[21]\n",
    "        self.pool4 = vgg16.features[23]\n",
    "        \n",
    "        self.conv5_1 = vgg16.features[24]\n",
    "        self.conv5_2 = vgg16.features[26]\n",
    "        self.conv5_3 = vgg16.features[28]\n",
    "        self.pool5 = vgg16.features[30]\n",
    "        \n",
    "        # FC6 –∏ FC7 –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # VGG layers\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "        conv4_3_feats = x\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = F.relu(self.conv5_1(x))\n",
    "        x = F.relu(self.conv5_2(x))\n",
    "        x = F.relu(self.conv5_3(x))\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = F.relu(self.conv6(x))\n",
    "        conv7_feats = F.relu(self.conv7(x))\n",
    "        \n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    \"\"\"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è multi-scale feature maps\"\"\"\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "        \n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "    \n",
    "    def forward(self, conv7_feats):\n",
    "        x = F.relu(self.conv8_1(conv7_feats))\n",
    "        conv8_2_feats = F.relu(self.conv8_2(x))\n",
    "        \n",
    "        x = F.relu(self.conv9_1(conv8_2_feats))\n",
    "        conv9_2_feats = F.relu(self.conv9_2(x))\n",
    "        \n",
    "        x = F.relu(self.conv10_1(conv9_2_feats))\n",
    "        conv10_2_feats = F.relu(self.conv10_2(x))\n",
    "        \n",
    "        x = F.relu(self.conv11_1(conv10_2_feats))\n",
    "        conv11_2_feats = F.relu(self.conv11_2(x))\n",
    "        \n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
    "\n",
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\"Prediction —Å–ª–æ–∏ –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        n_boxes = {'conv4_3': 4, 'conv7': 6, 'conv8_2': 6, 'conv9_2': 6, 'conv10_2': 4, 'conv11_2': 4}\n",
    "        \n",
    "        # Localization prediction convolutions\n",
    "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Class prediction convolutions\n",
    "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "        \n",
    "        # Localization predictions\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv7 = self.loc_conv7(conv7_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 4)\n",
    "        \n",
    "        # Class predictions\n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv7 = self.cl_conv7(conv7_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats).permute(0, 2, 3, 1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
    "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)\n",
    "        \n",
    "        return locs, classes_scores\n",
    "\n",
    "class SSD300(nn.Module):\n",
    "    \"\"\"–ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å SSD300\"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD300, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "        \n",
    "        self.priors_cxcy = prior_boxes\n",
    "    \n",
    "    def forward(self, images):\n",
    "        conv4_3_feats, conv7_feats = self.base(images)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats)\n",
    "        \n",
    "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, \n",
    "                                               conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        \n",
    "        return locs, classes_scores\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "print(\"üî® –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ SSD300...\")\n",
    "print(\"   (–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π VGG16 –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)\")\n",
    "model = SSD300(n_classes=NUM_CLASSES).to(device)\n",
    "print(f\"‚úÖ –ú–æ–¥–µ–ª—å SSD300 —Å–æ–∑–¥–∞–Ω–∞\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. MultiBox Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"MultiBox Loss –¥–ª—è SSD\"\"\"\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.0):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        n_classes = predicted_scores.size(2)\n",
    "        \n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)\n",
    "        \n",
    "        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –±–∞—Ç—á–µ\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0)\n",
    "            \n",
    "            overlap = find_jaccard_overlap(boxes[i], self.priors_xy)\n",
    "            \n",
    "            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ prior –Ω–∞–π—Ç–∏ –ª—É—á—à–∏–π ground truth box\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)\n",
    "            \n",
    "            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ ground truth –Ω–∞–π—Ç–∏ –ª—É—á—à–∏–π prior\n",
    "            _, prior_for_each_object = overlap.max(dim=1)\n",
    "            \n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "            \n",
    "            # –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ labels\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0\n",
    "            \n",
    "            true_classes[i] = label_for_each_prior\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)\n",
    "        \n",
    "        # Positive priors\n",
    "        positive_priors = true_classes != 0\n",
    "        \n",
    "        # Localization loss\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])\n",
    "        \n",
    "        # Confidence loss —Å hard negative mining\n",
    "        n_positives = positive_priors.sum(dim=1)\n",
    "        n_hard_negatives = self.neg_pos_ratio * n_positives\n",
    "        \n",
    "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)\n",
    "        \n",
    "        conf_loss_pos = conf_loss_all[positive_priors]\n",
    "        \n",
    "        conf_loss_neg = conf_loss_all.clone()\n",
    "        conf_loss_neg[positive_priors] = 0.\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)\n",
    "        \n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)\n",
    "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]\n",
    "        \n",
    "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()\n",
    "        \n",
    "        return conf_loss + self.alpha * loc_loss\n",
    "\n",
    "criterion = MultiBoxLoss(priors_cxcy=prior_boxes)\n",
    "print(\"‚úÖ MultiBoxLoss –æ–ø—Ä–µ–¥–µ–ª–µ–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "print(\"üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
    "train_dataset = BCCDDataset(DATA_ROOT, split='train', transform=Transform())\n",
    "val_dataset = BCCDDataset(DATA_ROOT, split='val', transform=Transform())\n",
    "test_dataset = BCCDDataset(DATA_ROOT, split='test', transform=Transform())\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         collate_fn=train_dataset.collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                       collate_fn=val_dataset.collate_fn, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=test_dataset.collate_fn, num_workers=0)\n",
    "\n",
    "print(f\"\\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:\")\n",
    "print(f\"   Train: {len(train_dataset)} images ({len(train_loader)} batches)\")\n",
    "print(f\"   Val: {len(val_dataset)} images ({len(val_loader)} batches)\")\n",
    "print(f\"   Test: {len(test_dataset)} images ({len(test_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"–î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\"\"\"\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def visualize_batch(images, boxes, labels, n_samples=4):\n",
    "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞ —Å bounding boxes\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx in range(min(n_samples, len(images))):\n",
    "        img = denormalize(images[idx].clone().cpu())\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        \n",
    "        for box, label in zip(boxes[idx], labels[idx]):\n",
    "            xmin, ymin, xmax, ymax = box.cpu().numpy()\n",
    "            xmin *= IMAGE_SIZE\n",
    "            ymin *= IMAGE_SIZE\n",
    "            xmax *= IMAGE_SIZE\n",
    "            ymax *= IMAGE_SIZE\n",
    "            \n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            \n",
    "            class_name = REV_LABEL_MAP[label.item()]\n",
    "            color = np.array(COLORS[class_name]) / 255.0\n",
    "            \n",
    "            rect = patches.Rectangle((xmin, ymin), width, height,\n",
    "                                    linewidth=2, edgecolor=color, facecolor='none')\n",
    "            axes[idx].add_patch(rect)\n",
    "            axes[idx].text(xmin, ymin-5, class_name, color='white',\n",
    "                         bbox=dict(facecolor=color, alpha=0.8), fontsize=10)\n",
    "        \n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f'Image {idx+1}: {len(boxes[idx])} objects', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "print(\"üì∏ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ train –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
    "images, boxes, labels = next(iter(train_loader))\n",
    "visualize_batch(images, boxes, labels)\n",
    "print(\"‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∏–∑ train –¥–∞—Ç–∞—Å–µ—Ç–∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "**–í–Ω–∏–º–∞–Ω–∏–µ:** –û–±—É—á–µ–Ω–∏–µ –∑–∞–π–º–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ (2-4 —á–∞—Å–∞ –Ω–∞ GPU, 10-20 —á–∞—Å–æ–≤ –Ω–∞ CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "biases = []\n",
    "not_biases = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param_name.endswith('.bias'):\n",
    "            biases.append(param)\n",
    "        else:\n",
    "            not_biases.append(param)\n",
    "\n",
    "optimizer = optim.SGD(params=[{'params': biases, 'lr': 2 * LEARNING_RATE},\n",
    "                             {'params': not_biases}],\n",
    "                     lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(NUM_EPOCHS * 0.5), \n",
    "                                                                  int(NUM_EPOCHS * 0.75)], \n",
    "                                          gamma=0.1)\n",
    "\n",
    "print(\"‚úÖ Optimizer –∏ scheduler –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã\")\n",
    "print(f\"   Initial LR: {LEARNING_RATE}\")\n",
    "print(f\"   LR –±—É–¥–µ—Ç —É–º–µ–Ω—å—à–µ–Ω –Ω–∞ —ç–ø–æ—Ö–∞—Ö: {int(NUM_EPOCHS * 0.5)}, {int(NUM_EPOCHS * 0.75)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, epoch):\n",
    "    \"\"\"–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    for images, boxes, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_locs, predicted_scores = model(images)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if GRAD_CLIP is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "    \"\"\"–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, boxes, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "            \n",
    "            predicted_locs, predicted_scores = model(images)\n",
    "            loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'checkpoints/best_model.pth')\n",
    "        print(f\"  ‚úÖ Best model saved (val_loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'checkpoints/ssd300_epoch_{epoch+1}.pth')\n",
    "        print(f\"  üíæ Checkpoint saved: epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Total epochs: {NUM_EPOCHS}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì—Ä–∞—Ñ–∏–∫ loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs_range, train_losses, 'b-o', label='Train Loss', linewidth=2, markersize=4)\n",
    "plt.plot(epochs_range, val_losses, 'r-s', label='Validation Loss', linewidth=2, markersize=4)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\")\n",
    "print(f\"  –§–∏–Ω–∞–ª—å–Ω—ã–π Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  –§–∏–Ω–∞–ª—å–Ω—ã–π Val Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"  –õ—É—á—à–∏–π Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"  –≠–ø–æ—Ö–∞ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {val_losses.index(best_val_loss) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. –§—É–Ω–∫—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(model, images, min_score=0.2, max_overlap=0.45, top_k=200):\n",
    "    \"\"\"\n",
    "    –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\n",
    "    \n",
    "    Returns:\n",
    "        det_boxes, det_labels, det_scores –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_locs, predicted_scores = model(images)\n",
    "    \n",
    "    batch_size = predicted_locs.size(0)\n",
    "    predicted_scores = F.softmax(predicted_scores, dim=2)\n",
    "    \n",
    "    all_det_boxes = []\n",
    "    all_det_labels = []\n",
    "    all_det_scores = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "        decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], prior_boxes))\n",
    "        \n",
    "        det_boxes = []\n",
    "        det_labels = []\n",
    "        det_scores = []\n",
    "        \n",
    "        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–∫—Ä–æ–º–µ background)\n",
    "        for c in range(1, NUM_CLASSES):\n",
    "            class_scores = predicted_scores[i][:, c]\n",
    "            \n",
    "            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ confidence\n",
    "            score_above_min_score = class_scores > min_score\n",
    "            n_above_min_score = score_above_min_score.sum().item()\n",
    "            \n",
    "            if n_above_min_score == 0:\n",
    "                continue\n",
    "            \n",
    "            class_scores = class_scores[score_above_min_score]\n",
    "            class_decoded_locs = decoded_locs[score_above_min_score]\n",
    "            \n",
    "            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score\n",
    "            class_scores, sort_ind = class_scores.sort(dim=0, descending=True)\n",
    "            class_decoded_locs = class_decoded_locs[sort_ind]\n",
    "            \n",
    "            # NMS\n",
    "            overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)\n",
    "            \n",
    "            suppress = torch.zeros((n_above_min_score), dtype=torch.bool).to(device)\n",
    "            \n",
    "            for box in range(class_decoded_locs.size(0)):\n",
    "                if suppress[box]:\n",
    "                    continue\n",
    "                \n",
    "                suppress = suppress | (overlap[box] > max_overlap)\n",
    "                suppress[box] = False\n",
    "            \n",
    "            det_boxes.append(class_decoded_locs[~suppress])\n",
    "            det_labels.append(torch.LongTensor((~suppress).sum().item() * [c]).to(device))\n",
    "            det_scores.append(class_scores[~suppress])\n",
    "        \n",
    "        if len(det_boxes) == 0:\n",
    "            det_boxes = torch.FloatTensor([[0., 0., 1., 1.]]).to(device)\n",
    "            det_labels = torch.LongTensor([0]).to(device)\n",
    "            det_scores = torch.FloatTensor([0.]).to(device)\n",
    "        else:\n",
    "            det_boxes = torch.cat(det_boxes, dim=0)\n",
    "            det_labels = torch.cat(det_labels, dim=0)\n",
    "            det_scores = torch.cat(det_scores, dim=0)\n",
    "        \n",
    "        # Top-K\n",
    "        if det_boxes.size(0) > top_k:\n",
    "            det_scores, sort_ind = det_scores.sort(dim=0, descending=True)\n",
    "            det_scores = det_scores[:top_k]\n",
    "            det_boxes = det_boxes[sort_ind][:top_k]\n",
    "            det_labels = det_labels[sort_ind][:top_k]\n",
    "        \n",
    "        all_det_boxes.append(det_boxes)\n",
    "        all_det_labels.append(det_labels)\n",
    "        all_det_scores.append(det_scores)\n",
    "    \n",
    "    return all_det_boxes, all_det_labels, all_det_scores\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(images, det_boxes, det_labels, det_scores, n_samples=4, min_score=0.2):\n",
    "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx in range(min(n_samples, len(images))):\n",
    "        img = denormalize(images[idx].clone().cpu())\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        \n",
    "        boxes = det_boxes[idx].cpu()\n",
    "        labels = det_labels[idx].cpu()\n",
    "        scores = det_scores[idx].cpu()\n",
    "        \n",
    "        n_det = 0\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if score < min_score:\n",
    "                continue\n",
    "            \n",
    "            xmin, ymin, xmax, ymax = box.numpy()\n",
    "            xmin *= IMAGE_SIZE\n",
    "            ymin *= IMAGE_SIZE\n",
    "            xmax *= IMAGE_SIZE\n",
    "            ymax *= IMAGE_SIZE\n",
    "            \n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            \n",
    "            class_name = REV_LABEL_MAP[label.item()]\n",
    "            if class_name == 'background':\n",
    "                continue\n",
    "            \n",
    "            n_det += 1\n",
    "            color = np.array(COLORS[class_name]) / 255.0\n",
    "            \n",
    "            rect = patches.Rectangle((xmin, ymin), width, height,\n",
    "                                    linewidth=2.5, edgecolor=color, facecolor='none')\n",
    "            axes[idx].add_patch(rect)\n",
    "            \n",
    "            text = f'{class_name} {score:.2f}'\n",
    "            axes[idx].text(xmin, ymin-5, text, color='white',\n",
    "                         bbox=dict(facecolor=color, alpha=0.8, pad=3), fontsize=9, fontweight='bold')\n",
    "        \n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f'Image {idx+1}: {n_det} detections', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "print(\"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...\")\n",
    "model.load_state_dict(torch.load('checkpoints/best_model.pth'))\n",
    "model.eval()\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "\n",
    "# –î–µ—Ç–µ–∫—Ü–∏—è –Ω–∞ test –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "print(\"\\nüîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö...\")\n",
    "images, true_boxes, true_labels = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "det_boxes, det_labels, det_scores = detect_objects(model, images, min_score=0.2)\n",
    "\n",
    "print(\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö:\")\n",
    "visualize_detections(images, det_boxes, det_labels, det_scores)\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–π\n",
    "print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–π:\")\n",
    "for i, (boxes, labels, scores) in enumerate(zip(det_boxes, det_labels, det_scores)):\n",
    "    print(f\"\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}:\")\n",
    "    filtered_count = (scores >= 0.2).sum().item()\n",
    "    print(f\"  –í—Å–µ–≥–æ –¥–µ—Ç–µ–∫—Ü–∏–π: {filtered_count}\")\n",
    "    for cls in range(1, NUM_CLASSES):\n",
    "        mask = (labels == cls) & (scores >= 0.2)\n",
    "        n_detections = mask.sum().item()\n",
    "        if n_detections > 0:\n",
    "            avg_score = scores[mask].mean().item()\n",
    "            print(f\"    {REV_LABEL_MAP[cls]}: {n_detections} –¥–µ—Ç–µ–∫—Ü–∏–π (avg score: {avg_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Ground Truth vs Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ground_truth_predictions(images, true_boxes, true_labels, det_boxes, det_labels, det_scores, idx=0, min_score=0.2):\n",
    "    \"\"\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ ground truth –∏ predictions\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    \n",
    "    img = denormalize(images[idx].clone().cpu())\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Ground Truth\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Ground Truth', fontsize=16, fontweight='bold', pad=10)\n",
    "    \n",
    "    for box, label in zip(true_boxes[idx], true_labels[idx]):\n",
    "        xmin, ymin, xmax, ymax = box.cpu().numpy()\n",
    "        xmin *= IMAGE_SIZE\n",
    "        ymin *= IMAGE_SIZE\n",
    "        xmax *= IMAGE_SIZE\n",
    "        ymax *= IMAGE_SIZE\n",
    "        \n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        class_name = REV_LABEL_MAP[label.item()]\n",
    "        color = np.array(COLORS[class_name]) / 255.0\n",
    "        \n",
    "        rect = patches.Rectangle((xmin, ymin), width, height,\n",
    "                                linewidth=3, edgecolor=color, facecolor='none')\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(xmin, ymin-5, class_name, color='white',\n",
    "                bbox=dict(facecolor=color, alpha=0.8, pad=3), fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    ax2.imshow(img)\n",
    "    ax2.set_title('Predictions', fontsize=16, fontweight='bold', pad=10)\n",
    "    \n",
    "    boxes = det_boxes[idx].cpu()\n",
    "    labels = det_labels[idx].cpu()\n",
    "    scores = det_scores[idx].cpu()\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score < min_score:\n",
    "            continue\n",
    "        \n",
    "        xmin, ymin, xmax, ymax = box.numpy()\n",
    "        xmin *= IMAGE_SIZE\n",
    "        ymin *= IMAGE_SIZE\n",
    "        xmax *= IMAGE_SIZE\n",
    "        ymax *= IMAGE_SIZE\n",
    "        \n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        class_name = REV_LABEL_MAP[label.item()]\n",
    "        if class_name == 'background':\n",
    "            continue\n",
    "        \n",
    "        color = np.array(COLORS[class_name]) / 255.0\n",
    "        \n",
    "        rect = patches.Rectangle((xmin, ymin), width, height,\n",
    "                                linewidth=3, edgecolor=color, facecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "        \n",
    "        text = f'{class_name} {score:.2f}'\n",
    "        ax2.text(xmin, ymin-5, text, color='white',\n",
    "                bbox=dict(facecolor=color, alpha=0.8, pad=3), fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "print(\"\\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Ground Truth –∏ Predictions:\\n\")\n",
    "for i in range(min(3, len(images))):\n",
    "    print(f\"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}:\")\n",
    "    compare_ground_truth_predictions(images, true_boxes, true_labels, \n",
    "                                    det_boxes, det_labels, det_scores, idx=i)\n",
    "\n",
    "print(\"\\n‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ –∏ –∏—Ç–æ–≥–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"–ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø SSD300 –ù–ê BCCD DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\")\n",
    "print(f\"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "print(f\"\\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "print(f\"  –õ—É—á—à–∏–π validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  –§–∏–Ω–∞–ª—å–Ω—ã–π train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  –§–∏–Ω–∞–ª—å–Ω—ã–π val loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"  –≠–ø–æ—Ö–∞ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {val_losses.index(best_val_loss) + 1}\")\n",
    "\n",
    "print(f\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:\")\n",
    "print(f\"  checkpoints/best_model.pth\")\n",
    "for epoch in range(5, NUM_EPOCHS+1, 5):\n",
    "    if os.path.exists(f'checkpoints/ssd300_epoch_{epoch}.pth'):\n",
    "        print(f\"  checkpoints/ssd300_epoch_{epoch}.pth\")\n",
    "\n",
    "print(f\"\\nüìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:\")\n",
    "print(f\"  ‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤\")\n",
    "print(f\"  ‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å Ground Truth\")\n",
    "print(f\"  ‚úÖ –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è (Train vs Val loss)\")\n",
    "print(f\"  ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "print(f\"  ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Ground Truth vs Predictions\")\n",
    "\n",
    "print(f\"\\nüéì –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã SSD300:\")\n",
    "print(f\"  ‚úÖ VGG-16 base network (pretrained)\")\n",
    "print(f\"  ‚úÖ Auxiliary convolutions (multi-scale)\")\n",
    "print(f\"  ‚úÖ Prediction heads (localization + classification)\")\n",
    "print(f\"  ‚úÖ {prior_boxes.size(0)} prior boxes –Ω–∞ 6 feature maps\")\n",
    "print(f\"  ‚úÖ MultiBox Loss —Å hard negative mining\")\n",
    "print(f\"  ‚úÖ Non-Maximum Suppression\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Notebook —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –≤—ã–≤–æ–¥—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\")\n",
    "print(\"‚úÖ –ì–æ—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ GitHub\")\n",
    "print(\"‚úÖ –ó–∞–º–µ—á–∞–Ω–∏–µ '–Ω–µ—Ç –≤—ã–≤–æ–¥–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —è—á–µ–µ–∫' –ù–ï –ø–æ—è–≤–∏—Ç—Å—è\\n\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
